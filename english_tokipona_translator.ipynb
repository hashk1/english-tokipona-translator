{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english-tokipona-translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOpZyZQ13OLnYDFSAMYeRzZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hashk1/english-tokipona-translator/blob/main/english_tokipona_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaOeMbquUQWn"
      },
      "source": [
        "# English-tokipona translator on Google Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwhCkCW3RZF2"
      },
      "source": [
        "### Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk43VxJ5kOPc"
      },
      "source": [
        "!pip install tatoebatools\n",
        "!git clone https://github.com/hashk1/tensor2tensor.git -b hashk1-fix-trainer-and-decoder\n",
        "!cd tensor2tensor && pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "790kmF9hRsd3"
      },
      "source": [
        "### Download data from Tatoeba"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yZfmdgk7y0w"
      },
      "source": [
        "from tatoebatools import ParallelCorpus, tatoeba\n",
        "tatoeba.dir = \"./data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FhHvFE18E-T"
      },
      "source": [
        "with open(\"english.txt\", \"w\") as f1:\n",
        "  with open(\"tokipona.txt\", \"w\") as f2:\n",
        "    for sentence, translation in ParallelCorpus(\"eng\", \"toki\"):\n",
        "      f1.write(sentence.text + \"\\n\")\n",
        "      f2.write(translation.text + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se3gocaGSI9U"
      },
      "source": [
        "### Add scripts to generate t2t formatted data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHST_mccPFED"
      },
      "source": [
        "!mkdir -p t2t\n",
        "!echo \"from . import myproblem\" > t2t/__init__.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWuTyQh3RGIG"
      },
      "source": [
        "code = \"\"\"from tensor2tensor.data_generators import problem\n",
        "from tensor2tensor.data_generators import text_problems\n",
        "from tensor2tensor.utils import registry\n",
        "\n",
        "@registry.register_problem\n",
        "class eng2toki(text_problems.Text2TextProblem):\n",
        "    @property\n",
        "    def approx_vocab_size(self):\n",
        "        return 2**13\n",
        "\n",
        "    @property\n",
        "    def is_generate_per_split(self):\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def dataset_splits(self):\n",
        "        return [{\n",
        "            \"split\": problem.DatasetSplit.TRAIN,\n",
        "            \"shards\": 9,\n",
        "        }, {\n",
        "            \"split\": problem.DatasetSplit.EVAL,\n",
        "            \"shards\": 1,\n",
        "        }]\n",
        "\n",
        "    def generate_samples(self, data_dir, tmp_dir, dataset_split):\n",
        "        filename_input = \"./english.txt\"\n",
        "        filename_output = \"./tokipona.txt\"\n",
        "\n",
        "        with open(filename_input) as f_in, open(filename_output) as f_out:\n",
        "            for src, tgt in zip(f_in, f_out):\n",
        "                src = src.strip()\n",
        "                tgt = tgt.strip()\n",
        "                if not src or not tgt:\n",
        "                    continue\n",
        "                yield {\"inputs\": src, \"targets\": tgt}\"\"\"\n",
        "                \n",
        "!echo '$code' > t2t/myproblem.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BM9mTwaBTeRw"
      },
      "source": [
        "### Run data generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCRK502C840u"
      },
      "source": [
        "!t2t-datagen \\\n",
        "  --data_dir=. \\\n",
        "  --tmp_dir=./t2t \\\n",
        "  --problem=eng2toki \\\n",
        "  --t2t_usr_dir=./t2t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubF3EuXLTkPJ"
      },
      "source": [
        "### Train the data\n",
        "- Default train steps is 250,000.\n",
        "- If you change steps, then add option `--train_steps=XXXX` ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvVBfFa-CDzu"
      },
      "source": [
        "!t2t-trainer \\\n",
        "  --data_dir=. \\\n",
        "  --problem=eng2toki \\\n",
        "  --model=transformer \\\n",
        "  --hparams_set=transformer_base_single_gpu \\\n",
        "  --output_dir=./t2t \\\n",
        "  --t2t_usr_dir=./t2t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu6MRQbeTsjf"
      },
      "source": [
        "### Run decoder\n",
        "- input: English, output: toki pona\n",
        "- q: quit the decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JVR2qEdCh3V"
      },
      "source": [
        "!t2t-decoder \\\n",
        "   --data_dir=./ \\\n",
        "   --problem=eng2toki \\\n",
        "   --model=transformer \\\n",
        "   --hparams_set=transformer_base_single_gpu \\\n",
        "   --output_dir=./t2t \\\n",
        "   --model_dir=./t2t \\\n",
        "   --decode_hparams=\"beam_size=4,alpha=0.6\" \\\n",
        "   --decode_interactive=true \\\n",
        "   --t2t_usr_dir=./t2t "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1UuKEhajgse"
      },
      "source": [
        "### Save model to Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKnTNCu_cW9i"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6w-a5WyhsjG"
      },
      "source": [
        "!mkdir -p drive/MyDrive/t2t/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGjz308cac-b"
      },
      "source": [
        "!cp -pr t2t/eval* drive/MyDrive/t2t/\n",
        "!cp -p t2t/*.py drive/MyDrive/t2t/\n",
        "!cp -p [cefgh]* drive/MyDrive/t2t/\n",
        "!cp -p model.ckpt-250000.* drive/MyDrive/t2t/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnSeDh9qaz9k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}